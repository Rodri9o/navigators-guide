
<!-- split to own file for ch05.1 -->
**Environments**
<!-- environments -->
  * dev
  * staging
  * prod

Before we jump into deploying Droplets and services all willy-nilly, let's go over our environments. In most serious projects you're going to have a dev, staging, and production environment to allow yourself room to tinker, test, and deploy without having to worry about bringing your live site down. Doing a little planning ahead of time will go a long way in preventing headaches. Terraform provides a workspace feature that keeps the terraform.tfstate files separate from one another. However, we're actually not going to use that since we're going to make use of Ansible for Droplet configuration after boot up and use directory structure to isolate environments. This can be done a few different ways and often times it comes down to personal preference or whatever works best for your project. Here's an example structure.

```
.
├── ansible.cfg
├── bin/
├── environments/
│   ├── config/
│   │   └── cloud-config.yaml
│   ├── dev/
│   ├── prod/
│   ├── staging/
│   │   ├── config -> ../config
│   │   ├── group_vars
│   │   ├── host_vars
│   │   └── terraform-inventory -> ../terraform-inventory
│   ├── symfiles/
│   │   └── manifest.json
│   ├── symvars/
│   │   ├── galera-cluster-node.yml
│   │   ├── galera-loadbalancer.yml
│   │   ├── README.md
│   │   ├── shared-group-vars.yml
│   │   └── shared-vault.yml
│   └── terraform-inventory
├── galera-deploy.yml
├── ghost-build.yml
├── packer-build.yml
└── roles/

```

One other thing to note is that we're not configuring Terraform to use remote state. Just know that when you're working with a team, you'll want to look into setting up a remote state backend like Consul which supports state locking. Okay, let's go over a few things to explain the logic behind this type of layout. The first is that we're keeping files that pertain to similar components in separate environments apart from one another. The important thing is that we don't want to run some Ansible or Terraform scripts on the wrong environment and bring everything crashing down. We do this by placing directories in the **environments** dir and each one gets its own subdirectory. Since we're using Terraform, we can place our individual scripts per environment in each directory, and we can go even further by placing different parts of your infrastructure in further subdirectories in order to isolate each of them from one another. Let's use your staging environment as an example. You can break down staging into the components it's comprised of like your database, file storage (Spaces), Load Balancer, Application Droplets and so on. There are actually some really great write-ups about this topic online, one of which has actually turned into the book, *"Terraform: Up & Running"* by Yevgeniy Brikman. You can check out his blog post which covers this in more detail: https://blog.gruntwork.io/a-comprehensive-guide-to-terraform-b3d32832baca

We're also going to make use of versioned Terraform modules in our setup. What this allows you to do is make changes to your infrastructure in staging without affecting production. You don't want to make a breaking change in production just by updating a resource configuration in one of you modules. Here's an example of what that would look like.

*staging*
```
module "sippin_db" {
  source           = "github.com/cmndrsp0ck/galera-tf-mod.git?ref=v1.0.4"
  project          = "${var.project}"
  region           = "${var.region}"
  keys             = "${var.keys}"
  private_key_path = "${var.private_key_path}"
  ssh_fingerprint  = "${var.ssh_fingerprint}"
  public_key       = "${var.public_key}"
  ansible_user     = "${var.ansible_user}"
}
```

*prod*
```
module "sippin_db" {
  source           = "github.com/cmndrsp0ck/galera-tf-mod.git?ref=v1.0.2"
  project          = "${var.project}"
  region           = "${var.region}"
  keys             = "${var.keys}"
  private_key_path = "${var.private_key_path}"
  ssh_fingerprint  = "${var.ssh_fingerprint}"
  public_key       = "${var.public_key}"
  ansible_user     = "${var.ansible_user}"
}
```

The only change in the previous 2 examples is the value assigned to the *ref* key at the end of the source line. You'll also notice that the arguments passed into the module are referencing variables set in **terraform.tfvars**. You can pass in strings within the same block that calls your modules if that's your preference. Let's take a look at the module to see what it's doing. If you were to clone the repo you'll see that it contains nothing more than a basic set of Terraform files that declare variables and create a set of resources. It's really nothing special to look at, however, using modules really allows you to

<!-- build out terraform module for load balancer -->
**Terraform module**

Let's take a look at the module to see what it's doing. If you were to clone the repo you'll see that it contains nothing more than a basic set of Terraform files that declare variables and create a set of resources.

<!-- build out database -->
<!-- build out single instance of ghost -->

**Speeding up the ability to scale with snapshots**

* Creating a template
* Templating software
  * Packer
  * Ansible is yet again useful when building your images

So far we've spun up a Load Balancer with a few dummy backends to give you an example of how easy it is to deploy a highly available service, but the dummy backends are not really useful in the real world. In most cases you'll have an application running on your backends that need to be ready for incoming requests as soon as they've been added to your Load Balancer's configuration. Spinning up base images and installing all dependencies and making configuration changes when you need to increase your application's capacity is not efficient. To speed things up and minimize the amount of work required to add new backends you can simply create your own images with all of your software dependencies pre-baked in leaving you to only configure the application when the Droplet is provisioned.

Creating a template can be manual process if you'd like. That means spinning up a single Droplet, logging in and running through all of the steps one-by-one, testing it out, then finally creating a snapshot. This isn't very practical though since it's a slow, error prone process. A step in the right direction would be to script the process. You can use whatever language you're comfortable with. Often times using bash scripts works well for simple builds but keep in mind that as your project requirements grow, your scripts will also as will the effort to maintain those scripts.

Another option would be to use server templating tool like Packer. Now while Packer isn't going to configure your image for you on its own, it does allow you to easily create a repeatable and testable process. Packer works with the scripts you already have and supports a large set of additional configuration tools including Chef, Puppet, Salt, and Ansible. It also has the ability to work with tons of providers and is able to generate multiple image formats including Docker. For our example we're going to make use of Packer's Ansible remote provisioner to create an image with a Ghost blog and all of its dependencies installed.

Packer templates are written using JSON to describe the builds it will perform. It's a straightforward approach that allows you to get your images created quickly. For an in-depth view of the components that can be used in a template file check out https://www.packer.io/docs/templates/index.html. Here's the example template that we'll be using.

**ghost-node.json**
```json
{
    "builders": [
        {
            "type": "digitalocean",
            "api_token": "{{user `do_api_token`}}",
            "image": "{{user `app_node_image`}}",
            "region": "sfo2",
            "size": "s-1vcpu-1gb",
            "private_networking": true,
            "monitoring": true,
            "user_data_file": "./config/cloud-config.yaml",
            "snapshot_name": "{{user `project_name`}}_{{isotime \"06-01-02-03-04-05\"}}",
            "communicator": "ssh",
            "ssh_username": "root"
        }
    ],
    "provisioners": [
        {
            "type": "ansible",
            "playbook_file": "packer-build.yml",
            "ansible_env_vars": [ "ANSIBLE_HOST_KEY_CHECKING=False", "ANSIBLE_SSH_ARGS='-o ForwardAgent=yes -o ControlMaster=auto -o ControlPersist=60s'"],
            "extra_arguments": ["-vvv"],
            "inventory_directory": "{{template_dir}}",
            "user": "root"
        }
    ],
    "post-processors": [
        {
            "type": "manifest",
            "output": "manifest.json",
            "strip_path": true
        }
    ]
}
```

So the first part we're describing is the builder. This is responsible for declaring what type of image will be produced, and in our case that means the cloud provider we'll be setting the image up with. Each builder takes a number of arguments to set what base image to use, the Droplet size, region availability, pass in user-data, set the snapshot name and any connection settings. Also note that Packer allows you to set variables and has some built-in functions that cab be used throughout the template file. Anything that is placed within `{{ }}` is run through the packer template engine. For a full listing of functions check out https://www.packer.io/docs/templates/engine.html. You'll notice that some of the values are variables, but the variables are not listed in this file. You're able to create a separate file in order to store variables and pass the file to Packer as a command-line argument when executing your template by using `-var-file=`. This allows you to set the file name in your **.gitignore** so it doesn't get sent up to your repo. You don't really have to place base image type, or the project name in this file along with the API token, but for the sake of organization we'll keep all the variables in one file. Here's an example variable file.

**variable.json**
```json
{
	"app_node_image": "debian-9-x64",
	"do_api_token": "1r7l8dsmd6g09g56qdwakvkjzvn4q046wwfolqeputcgz5og26vyheg781f5bvbz",
	"project_name": "nav-guide"
}
```

We're passing in a cloud-config.yaml file just to make sure that we install any dependencies for our provisioner, wihch in this instance is Ansible. We're using Ansible to keep things organized, clean, and should then need arise, you can run some playbooks later on your existing infrastructure and know that steps won't be ran again since it's idempotent. Packer is going to take care of create the Droplet and supplying the private key and inventory to Ansible. All you need to do is toss in any flags you think are necessary and you're good to go.

The last section is not required, but in our case we'll make use of the manifest post-processor later on. This is simply going to output a listing of completed builds that Packer has created and store them in a file. We'll use this later on to grab the snapshot ID.
